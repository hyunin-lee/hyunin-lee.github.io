---
layout: page
title: ""
---

<div style="display: flex; align-items: center;">
  <img src="/assets/hyunin4.jpg" alt="Hyunin Lee" style="width: 25%; height: auto; border: 2px solid black; border-radius: 15px; margin-right: 20px;">
  <div>
    <h2 style="margin: 0;">Hyunin Lee</h2>
    <p style="margin: 0;">Ph.D at UC Berkeley</p>
    <p><strong>Contact:</strong> hyunin(at)berkeley(dot)edu</p>
    <p><strong>Research Interests:</strong> Reinforcment Learning </p>
    <p>
      <a href="./assets/CV_update.pdf" target="_blank">
        <img src="/assets/CV2.svg" alt="CV" style="width: auto; height: 30px; margin-right: 15px;">
      </a>
      <a href="https://scholar.google.com/citations?user=kHTDu1YAAAAJ&hl=en" target="_blank">
        <img src="/assets/googlescholar.svg" alt="Google Scholar" style="width: auto; height: 30px; margin-right: 15px;">
      </a>
      <a href="https://kr.linkedin.com/in/hyunin-lee-539b641b1" target="_blank">
        <img src="/assets/linkedin.svg" alt="LinkedIn" style="width: auto; height: 30px; margin-right: 15px;">
      </a>
      <a href="https://github.com/hyunin-lee" target="_blank">
        <img src="/assets/github.svg" alt="GitHub" style="width: auto; height: 30px;">
      </a>
    </p>
  </div>
</div>

<!-- 
    <p>
      <a href="https://scholar.google.com/citations?user=kHTDu1YAAAAJ&hl=en">Google Scholar</a> /
      <a href="https://kr.linkedin.com/in/hyunin-lee-539b641b1">LinkedIn</a> /
      <a href="https://github.com/hyunin-lee">GitHub</a>
    </p>
-->


<!--
<div style="display: flex; align-items: center;">
  <div style="width: 30%; height: auto; overflow: hidden; border: 2px solid black; border-radius: 15px; display: flex; justify-content: center; align-items: center;">
    <img src="/assets/hyunin2.jpg" alt="Hyunin Lee" style="width: 100%; height: auto; object-fit: cover;">
  </div>
  <div style="margin-left: 20px;">
    <h2 style="margin: 0;">Hyunin Lee</h2>
    <p style="margin: 0;">Ph.D at UC Berkeley & co-founder of OUTTA </p>
    <p><strong>Contact:</strong> hyunin(at)berkeley(dot)edu</p>
    <p><strong>Research Interests:</strong> Reinforcment Learning </p>
  </div>
</div>
-->
### About
I am Hyunin Lee, a rising third-year M.S./Ph.D. student in Mechanical Engineering at UC Berkeley. I am interested in **developing reinforcement learning algorithms and exploring their key concepts**. I am fortunate to work with [Somayeh Sojoudi](https://people.eecs.berkeley.edu/~sojoudi/index.html) and [Javad Lavaei](https://lavaei.ieor.berkeley.edu/). Prior to joining Berkeley, I received my B.S. degree from Seoul National University where I have worked on research projects within neuroscience and machine learning with [Yong-Lae Park](https://softrobotics.snu.ac.kr/). 

Besides my research, I am interested in AI education üìöüë©üèª‚Äçüíª. I cofound and currently advise [OUTTA](https://outta.ai/).

<!--
[Google scholar](https://scholar.google.com/citations?user=kHTDu1YAAAAJ&hl=en) / [Linkedin](https://kr.linkedin.com/in/hyunin-lee-539b641b1) / [Github](https://github.com/hyunin-lee)
-->

### Research questions
* What is the optimal design of the agent and environment in reinforcement learning?
* How can we develop learning algorithms based on the design of the agent and environment?
* What are the key bottlenecks preventing reinforcement learning from wider real-world applications?

### News 
* [2024.07] I am attending RLC.
* [2024.07] I am serving on the program chair committee for AAAI 2025.  
* [2024.07] New paper on reinforcement Learning: [A Hypothesis on Black Swan in Unchanging Environments](./assets/blackswanHumanMDP.pdf)
* [2024.05] New paper on Safe reinforcement Learning: [Policy-based Primal-Dual Methods for Concave CMDP with Variance Reduction](./assets/convexCMDP.pdf)
* [2024.05] New paper on Non-stationary Reinforcement Learning in **ICML 2024** <span style="color:red;"><b>(Oral, top 1%)</b></span>: [Pausing Policy Learning in Non-stationary Reinforcement Learning](./assets/ICML2024RL_hyunin.pdf). [[Talk](https://icml.cc/virtual/2024/session/35272)](https://icml.cc/virtual/2024/oral/35461))/[codes](https://github.com/hyunin-lee/ForecasterSAC)]
* [2024.04] I received a Berkeley research fellowship.
* [2024.03] I have served as a reviewer for ICML 2024.
* [2024.02] New paper on Reinforcement Learning in **IEEE TAC**: [Beyond Exact Gradients: Convergence of Stochastic
Soft-Max Policy Gradient Methods with Entropy Regularization](./assets/TAC_Entropy_SPG.pdf).
* [2023.10] I received a NeurIPS scholar award
* [2023.08] OUTTA has launched a new spinoff team, **AI PLAYGROUND**, dedicated to creating intuitive AI educational materials for elementary education! I lead the team. 
* [2023.05] New paper on Non-stationary Reinforcement Learning in **NeurIPS 2023**: [Tempo Adaption in Non-stationary Reinforcement Learning](./assets/TempoAdaption_NSRL.pdf). [[slides](./assets/TempoAdaption_NSRL_slides.pdf)] [[codes](https://github.com/hyunin-lee/TempoRL)]
* [2023.01] New paper on Causal Machine Learning in **CDC 2023**: [Initial State Interventions for Deconfounded Imitation Learning](https://sam.pfrommer.us/wp-content/uploads/2023/03/main.pdf).
* [2022.08] I started Ph.D. at UC Berkeley.
* [2022.05] I received the Kwanjeong Education Foundation Scholarship.
* [2022.03] New paper on Neuroscience and Machine Learning in **IEEE TNSRE**: [Explainable Deep Learning Model for EMG-Based Finger Angle Estimation using Attention](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9829861). [[slides](./assets/Explainable_EMG.pdf)] [[videos](https://www.youtube.com/watch?v=yYV5koXMPzo)] [[codes](https://github.com/hyunin-lee/AttentionEMG)]
* [2022] I have co-founded üöÄ[**OUTTA**](https://outta.ai/)üöÄ with [Haeun (MIT)](https://www.linkedin.com/in/david-ha-eun-kang-78b932132/), [Chankyo (UMich)](https://www.linkedin.com/in/chankyo-kim-603592238/)

### Education 
* Ph.D., University of California, Berkeley, 2022.08 - current
* B.S., Seoul National University, 2015.03 - 2022.02  
  (Served military service in Korean Combat Traning Center, 2017.07 - 2019.03)

